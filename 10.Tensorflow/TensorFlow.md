# TensorFlow 



벡터 : 1차원 

행렬 : 다차원 벡터 

텐서 : 벡터, 행렬을 다 통틀어서 말하는 것  텐서플로우에서 데이터의 기본단위이다. 



노드 - 수치 연산 

엣지 - 녿드 사이를 이동하는 다차원 데이터 텐서 





## 특징 및 장점 

머신러닝 안에 딥러닝이 포함되는 것 

TPU : tensorflow 전용 연산 플로우 (Tensor Processing Unit)





tensorflow 때문에 그래픽 카드 살 거면 mbdi 거가 좋음 



흐름

user code -> keras -> tensorflow 

`tf.__version__` 





데이터 불러오기 - 전처리 - 모델 생성 (우리가 생성하는 것) - 훈련 - 예측 



## 모델 구성 

- 층을 구성한 다음 모델을 컴파일 해야 함 

- Sequential API // 함수형 API 로 만든다 

- 보통은 층 API로 생성한다 

  - keras.Sequential()

  - 여기에 layer를 하나씩 하나씩 추가하는 것 

  - layer의 종류가 여러가지 있음 

    - Flatten : 데이터를 넓게 펴는 것 

      정사각형 형태의 5X5의 데이터를 -> 길이가 5X5인 행 한 줄로 만드는 것 

      이 때, 피쳐의 개수는 25개라고 할 수 있는 것임 (25차원의 데이터가 되는 것)

    - Dense : 뉴런에 들어오는 각각의 입력값의 가중치가 존재해서, 

      한 레이어에 뉴런이 N개가 있고, activation 함수(ex Relu)가 있는 것 

      해당 레이어의 뉴런 한 개가 각각 그 전 레이어의 모든 데이터를 받아옴 

      위의 flatten에서 25개였으니깐, 25개가 다 들어와서 가중치가 25개임 

      NX25개의 가중치가 생김 이정도의 수치연산을 해야 하는 것 (그래서 GPU를 사용하는 것)

      질문할 점: dense layer의 뉴런의 개수를 어떻게 정하나요?  기준이 따로 있는 건가요? => hyper parameter들은 감으로...한다고 한다.. (layer의 수, 뉴런의 개수, activation 함수 종류 등)

    - 마지막 단의 layer : 예시에서는 dense 사용했음 

      마지막 layer의 activation 함수는 보통 **softmax 함수** 사용함 

      마지막 layer의 뉴런이 10개였으니깐 각각 10개로 나옴 

      각각 0,1,2,...9번 클래스에 속할 확률이 나온다. 이걸 다 더하면 1이 됨 

      제일 확률이 높은 애가 이 이미지가 속할 클래스인 것임 

  ### 모델 컴파일

  - 손실함수는 무엇을 사용할 것인가? 
    - 모델을 훈련했을 때, error값을 어떤 방법으로 측정할 것인지가 loss function임 
    - 처음에는 parameter가 random하게 들어가기 때문에 오차가 크게 나옴 
    - 오차가 크게 나오면, 훈련을 계속하면서 가중치값들을 바꾸는데, loss fuction을 최소화하는 방향으로 하는 것 
    - 하지만, 손실함수도 문제에 따라서 종류가 달라짐 
  - 옵티마이저 : 데이터와 손실함수를 바탕으로 결과를 보고 가중치를 조정해주는 것 
  - 지표 :  훈련단계와 테스트 단계를 모니터링하기위해 사용 



### 모델 훈련 

- epochs : 

  60000만장이 모델에 한 번 들어가면 epochs가 1이라는 것 

  다섯 번 들어가면 5 

  이게 왜 의미가 있는 것인지? 처음에 모델 안의 parameter 값들이 random하게 설정되어서 훈련하게 되기 때문이다. 그런 경우, 결과로 나온 것의 loss function 값이 엄청 클 것임. 그래서 다시 들어가게 되면, 처음보다 파라미터 값들이 어느 정도 안정이 되어 있을 것이기 때문에, loss function이 낮아질 것임 

  큰 값을 주면 좋지만, 그렇다고 무조건 큰 값을 주는 것이 좋은 것은 아님 

  왜냐하면, 연산의 수가 epochs만큼 늘어나기 때문이다. 



### 정확도 평가

궁금한 점 : layer 종류, 손실함수 종류, 옵티마이저 종류, 지표 종류 









# CNN 

Fully-Connected Layer만으로 구성된 인공신경망의 입력 데이터는 1차원 형태로 한정 -> 정보 손실 발생 

: 28X28의 데이터를 한 줄로 만들면, (2차원을 1차원으로) 공간 정보 손실 



그래서 그 공간정보를 그대로 가지고 학습이 가능한 **CNN**을 사용한다. 

- 각 레이어의 입출력 데이터의 형상 유지 
- 이미지의 공간 정보를 유지하면서 인접 이미지와의 특징을 효과적으로 인식 
- 복수의 필터로 이미지의 특징 추출 및 학습 
- 추출한 이미지의 특징을 모으고 강화하는 Polling layer를 같이 사용 
- 필터를 공유 파라미터로 사용하기 때문에, 일반 인공 신경ㅁㅇ과 비교하여 학습 파라미터가 매우 적음 



## 2가지 파트 

특징 추출 파트 : convolution이 필터이고, Polling과 함께 사용 (Convolution Neural Network으로 구성됨)

클래스 분류 파트 : 히든레이어, 아웃풋 (Fully connected layer로 구성됨)

필터란? 해당 파트 안에 내가 찾는 패턴이 있는지 없는지를 확인하는 것 



**특징 추출 영역** 

- convolution layer (필수) 
- polling layer 

여기서는 input 데이터 형태, 즉 2차원 형태로 계쏙 간다. 

근데 클래스 분류 파트는 1차원으로 변형되어야 하기 때문에, 특징 추출 부분과 클래스 분류 부분 사이에 flatten layer가 들어가게 된다. 



## 주요 용어 

- Convolution (합성곱)

  : 데이터 전체를 보는 것이 아니라 부분을 보는 것이 핵심 아이디어 

  `부분`에 해당하는 것을 filter라고 함 

  - 만약에 그 부분이 3X3의 형태이면, 그 3X3을 훑어서 한 개의 값을 출력함 
  - 그 다음에 부분의 위치를 옮겨줌 -> 또 한 개의 값을 출력함 
  - 옮기는 양을 `stride`라고 함 
  - ex) 7X7의 이미지를 3X3 filter를  stride 1로 사용한다면? 5X5의 결과값이 나옴  
  - 그리고 output에는 각각 계산 결과값이 들어감 

  각 필터마다 convolution layer를 얻을 수 있음 

  feature map(convolution으로 만들어진 행렬) -> activation map(feature 맵 행렬에 활성함수를 적용한 결과) 까지 포함한 게 합성곱 임 



- channel 

  - 컬러이미지는 3개의 채널 (RGB)로 구성 
  - 흑백이미지는 1개의 채널 (명암)

  만약에 31(w)X39(h)이미지라면 input data의 shape가 각각 (39,31,3), (39,31,1)

  1개의 feature맵이 channel이 됨 



- Filter(kernel이라고 하기도 함)
  - 3X3, 4X4를 많이 사용함 
  - CNN에서 학습 대상은 필터의 파라미터 (가중치)임 
  - 같은 layer에 있는 필터들은 같은 크기어야 한다 



- Padding 
  - 7X7이 5X5가 될 때, 데이터 손실이 크니깐 output도 7X7형태로 맞춰주기 위해서 패딩을 사용하기도 함 



- Pooling 

  - Max pooling : 가장 큰 값이 output으로 
  - Mean pooling : 평균 값이 output으로 
  - Pooling layer 특징 
    - 학습 대상 파라미터가 없음
    - 여길 통과하면 행렬 크기가 감소 
    - 여길 통해 채널 수 변경은 없음 
  - Pooling의 목적 : 오버피팅 방지 
  - feature 수가 늘어나면 오버피팅의 가능성이 커지기 때문에, 패딩과 풀링을 통해 적절히 조절 
  - 그래서 합성곱 레이어 지나면 피쳐가 확 늘어난 것을 풀링으로 줄여주고 다시 합성곱~피쳐레이어 이런식으로 함 

  

  

  

  





